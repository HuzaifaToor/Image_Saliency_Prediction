# Image_Saliency_Prediction
  In the area of computational modelling, so-called visual attention models (also called saliency algorithms) can predict the gaze locations of human viewers. The general goal of this paper is to perform a comparison between multiple saliency models. As for the image database there are no fixation maps from a subjective test with people available, saliency maps of the images from the database are computed using a top performing DeepGaze II saliency model [1] and are defined as a ground truth for the further steps of the paper. At first, 100 images are selected for all further analysis described in this paper. The selected images do represent a broad range of visual complexity. The complexity is assessed with a simple measurement of how many white contents the respective ground truth saliency map is containing. In the next step, three saliency models are applied on the selected images. For this purpose, the models SAM, SalGAN and ML-Net are used, which are available as open source. After computing the respective saliency maps, the results of the used models are compared to the ground truth saliency maps computed with the DeepGaze II algorithm. For doing so, several popular saliency map performance metrics like Kullback-Leibler Divergence, AUC-Judd and SIM are applied to the computed saliency maps.
